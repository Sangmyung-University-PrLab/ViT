import torch from torch import nn import torch.nn.functional as F from torch import Tensor from einops import rearrange, reduce, repeat from einops.layers.torch import Rearrange, Reduce from torchsummary import summary import numpy as np  # To handle 2D images, reshape the image into a sequence of flattened 2D patches. class PatchEmbedding(nn.Module):     def __init__(self, in_channels=3, patch_size=16, emb_size=768, img_size=224):         super().__init__()         self.patch_size = patch_size          # # Method 1: Flatten and FC layer         # self.projection = nn.Sequential(         #     Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=patch_size, s2=patch_size),         #     nn.Linear(path_size * patch_size * in_channels, emb_size)         # )          # Method 2: Conv         self.projection = nn.Sequential(             # using a conv layer instead of a linear one -> performance gains             nn.Conv2d(in_channels, emb_size, patch_size, stride=patch_size),             Rearrange('b e (h) (w) -> b (h w) e')         )          self.cls_token = nn.Parameter(torch.randn(1,1,emb_size))         self.positions = nn.Parameter(torch.randn((img_size // patch_size) ** 2 + 1, emb_size))      def forward(self, x):         b = x.shape[0]         x = self.projection(x)         cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b) # 1,1,768 -> 16,1,768         # prepend the cls token to the input         x = torch.cat([cls_tokens, x], dim=1) #16,197,768         # add position embedding to prejected patches         x += self.positions         return x  # MultiHeadAttention class MultiHeadAttention(nn.Module):     def __init__(self, emb_size=768, num_heads=8, dropout=0):         super().__init__()         self.emb_size = emb_size         self.num_heads = num_heads         self.keys = nn.Linear(emb_size, emb_size)         self.queries = nn.Linear(emb_size, emb_size)         self.values = nn.Linear(emb_size, emb_size)         self.att_drop = nn.Dropout(dropout)         self.projection = nn.Linear(emb_size, emb_size)      def forward(self, x, mask=None):         # split keys, queries and values in num_heads         queries = rearrange(self.queries(x), 'b n (h d) -> b h n d', h=self.num_heads)  # b, 197, 768 -> b, 8, 197, 96         keys = rearrange(self.keys(x), 'b n (h d) -> b h n d', h=self.num_heads) # b, 197, 728 -> b, 8, 197, 91 ??         values = rearrange(self.values(x), 'b n (h d) -> b h n d', h=self.num_heads)         # sum up over the last axis, b,h,197,197 -> Q * K : 내적         energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)  # batch, num_head, query_len, key_len          if mask is not None:             fill_value = torch.finfo(torch.float32).min             energy.mask_fill(~mask, fill_value)          # Get Attention Score         scaling = self.emb_size ** (1 / 2)         att = F.softmax(energy, dim=-1) / scaling         att = self.att_drop(att)         # sum up over the third axis -> Attention Score * Values         out = torch.einsum('bhal, bhlv -> bhav', att, values)  # 197x91         out = rearrange(out, 'b h n d -> b n (h d)')         out = self.projection(out)         return out  # perform the residual addition. class ResidualAdd(nn.Module):     def __init__(self, fn):         super().__init__()         self.fn = fn      def forward(self, x, **kwargs):         res = x         x = self.fn(x, **kwargs)         x += res         return x   # Subclassing nn.Sequential to avoid writing the forward method. class FeedForwardBlock(nn.Sequential):     def __init__(self, emb_size, expansion=4, drop_p=0.):         super().__init__(             nn.Linear(emb_size, expansion * emb_size),             nn.GELU(),             nn.Dropout(drop_p),             nn.Linear(expansion * emb_size, emb_size),         )  # Create the Transformer Encoder Block class TransformerEncoderBlock(nn.Sequential):     def __init__(self, emb_size=768, drop_p=0., forward_expansion=4, forward_drop_p=0., **kwargs):         super().__init__(             ResidualAdd(nn.Sequential(                 nn.LayerNorm(emb_size),                 MultiHeadAttention(emb_size, **kwargs),                 nn.Dropout(drop_p)             )),             ResidualAdd(nn.Sequential(                 nn.LayerNorm(emb_size),                 FeedForwardBlock(emb_size, expansion=forward_expansion, drop_p=forward_drop_p),                 nn.Dropout(drop_p)             ))         )  # TransformerEncoder consists of L blocks of TransformerBlock class TransformerEncoder(nn.Sequential):     def __init__(self, depth=12, **kwargs):       #앞에 *이 붙은 이유는 인자를 리스트형식으로 보내는게 아니라 각각 나눠서 보내야함.       #예를 들면 인자를 [1,2,3]으로 넣을 경우 함수에서는 [1,2,3]으로 받지만       #*[1, 2, 3]일 경우 1, 2, 3 으로 각각 나눠진 후 들어갑니다.         super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])  # define ClassificationHead which gives the class probability # fine tuning 할때 사용 class ClassificationHead(nn.Sequential):     def __init__(self, emb_size=768, n_classes = 10):         super().__init__(             Reduce('b n e -> b e', reduction='mean'),             nn.LayerNorm(emb_size),             nn.Linear(emb_size, n_classes)             )  # pre-training 할때 사용 class PreTrainHead(nn.Sequential):     def __init__(self, emb_size=768, n_classes = 10):         super().__init__(             Reduce('b n e -> b e', reduction='mean'),             nn.LayerNorm(emb_size),              nn.Linear(emb_size, 120),              nn.Linear(120,100),              nn.Linear(100, n_classes),             )  # Define the ViT architecture class ViT(nn.Sequential):     def __init__(self, in_channels=3, patch_size=16, emb_size=768, img_size=224, depth=12, n_classes=10, **kwargs):         super().__init__(             PatchEmbedding(in_channels, patch_size, emb_size, img_size),             TransformerEncoder(depth, emb_size=emb_size, **kwargs),             # ClassificationHead(emb_size, n_classes)             PreTrainHead(emb_size, n_classes)         )

