{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Swin Transformer.ipynb","provenance":[],"authorship_tag":"ABX9TyMUmWtk3hGmE8Od987hiy04"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install einops\n","!pip install timm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hYRVUU0f4Rou","executionInfo":{"status":"ok","timestamp":1645445192202,"user_tz":-540,"elapsed":8678,"user":{"displayName":"유호준","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09086645003143885896"}},"outputId":"28b08c72-e971-4c1a-ed09-94f7c59fe7d1"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting einops\n","  Downloading einops-0.4.0-py3-none-any.whl (28 kB)\n","Installing collected packages: einops\n","Successfully installed einops-0.4.0\n","Collecting timm\n","  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n","\u001b[K     |████████████████████████████████| 431 kB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.11.1+cu111)\n","Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.10.0+cu111)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.10.0.2)\n","Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.5)\n","Installing collected packages: timm\n","Successfully installed timm-0.5.4\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import einops\n","import torch.utils.checkpoint as checkpoint\n","from timm.models.layers import DropPath, trunc_normal_\n","from torchsummary import summary"],"metadata":{"id":"gtJ7UGwJ4N_X","executionInfo":{"status":"ok","timestamp":1645445199061,"user_tz":-540,"elapsed":6862,"user":{"displayName":"유호준","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09086645003143885896"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["class Mlp(nn.Module):\n","  def __init__(self, in_features, hidden_features = None, out_features = None, act_layer = nn.GELU, drop = 0.):\n","    super().__init__()\n","    out_features = out_features or in_features\n","    hidden_features = hidden_features or in_features\n","    self.fc1 = nn.Linear(in_features, hidden_features)\n","    self.act = act_layer()\n","    self.fc2 = nn.Linear(hidden_features, out_features)\n","    self.drop = nn.Dropout(drop)\n","\n","  def forward(self,x):\n","    x = self.fc1(x)\n","    x = self.act(x)\n","    x = self.drop(x)\n","    x = self.fc2(x)\n","    x = self.drop(x)\n","    return x"],"metadata":{"id":"NLwYidRiL8BB","executionInfo":{"status":"ok","timestamp":1645437549792,"user_tz":-540,"elapsed":12,"user":{"displayName":"유호준","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09086645003143885896"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["class WindowAttention(nn.Module):\n","  def __init__(self,dim,window_size,num_heads,qkv_bias=True,qk_scale = None, attn_drop = 0., proj_drop = 0.):\n","    super().__init__()\n","    self.dim = dim\n","    self.window_size = window_size\n","    self.num_heads = num_heads\n","    head_dim = dim // num_heads\n","    self.scale = qk_scale or head_dim ** -0.5\n","\n","    # define a parameter table of relative position bias\n","    self.relative_position_bias_table = nn.Parameter(\n","            torch.zeros((2 * window_size - 1) * (2 * window_size - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n","\n","    # get pair-wise relative position index for each token inside the window\n","    coords_h = torch.arange(self.window_size)\n","    coords_w = torch.arange(self.window_size)\n","    coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n","    coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n","    relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n","    relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n","    relative_coords[:, :, 0] += self.window_size - 1  # shift to start from 0\n","    relative_coords[:, :, 1] += self.window_size - 1\n","    relative_coords[:, :, 0] *= 2 * self.window_size - 1\n","    relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n","    self.register_buffer('relative_position_index', relative_position_index)\n","\n","    self.qkv = nn.Linear(dim, dim*3, bias = qkv_bias)\n","    self.attn_drop = nn.Dropout(attn_drop)\n","    self.proj = nn.Linear(dim,dim)\n","    self.proj_drop = nn.Dropout(proj_drop)\n","\n","    trunc_normal_(self.relative_position_bias_table, std=.02)\n","    self.softmax = nn.Softmax(dim=-1)\n","\n","  def forward(self,x,mask=None):\n","    b,n,c = x.shape\n","    qkv = self.qkv(x).chunk(3,dim=-1)\n","    q,k,v = [einops.rearrange(t,'b n (h c) -> b h n c',h=self.num_heads) for t in qkv]\n","\n","    q = q * self.scale\n","    attn = torch.einsum('bhqd, bhkd -> bhqk', q,k) \n","    \n","    relative_position_bias = self.relative_position_bias_tableo[self.relative_positin_index.view(-1)].view(self.window_size**2, self.window_size**2, -1)#wh*ww, wh*ww, nh\n","    relative_position_bias = relative_position_bias.permute(2,0,1) # nh, wh*ww,wh*ww\n","    attn = attn + relative_position_bias.unsqueeze(0)\n","\n","    if mask is not None:\n","      nW = mask.shape[0]\n","      attn = attn.reshape(b // nW, nW, self.num_heads, n,n) + mask.unsqueeze(1).unsqueeze(0)\n","      attn = attn.view(-1, self.num_heads,n,n)\n","      attn = self.softmax(attn)\n","    else:\n","      attn = self.softmax(attn)\n","    attn = self.attn_drop(attn)\n","    x = torch.einsum('bhan,bhnv -> bhav',attn,v).reshape(b,-1,c)\n","    x = self.proj(x)\n","    x = self.proj_drop(x)\n","    return x\n"],"metadata":{"id":"XANp8gXpMgOc","executionInfo":{"status":"ok","timestamp":1645438080851,"user_tz":-540,"elapsed":274,"user":{"displayName":"유호준","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09086645003143885896"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","execution_count":5,"metadata":{"id":"b4OpxpUV3jZV","executionInfo":{"status":"ok","timestamp":1645437549792,"user_tz":-540,"elapsed":7,"user":{"displayName":"유호준","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09086645003143885896"}}},"outputs":[],"source":["class PatchEmbed(nn.Module):\n","  def __init__(self, img_size = 224, patch_size=4, in_channels=3,embed_dim=96, norm_layer = None):\n","    super().__init__()\n","    patch_resolution = [img_size//patch_size, img_size//patch_size]\n","    self.img_size = img_size\n","    self.patch_size = patch_size\n","    self.patch_resolution = patch_resolution\n","    self.num_patches = patch_resolution[0] * patch_resolution[1]\n","\n","    self.in_channels = in_channels\n","    self.embed_dim = embed_dim\n","\n","    self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size = patch_size, stride = patch_size)\n","    if norm_layer is not None:\n","      self.norm = norm_layer(embed_dim)\n","    else:\n","      self.norm = None\n","\n","  def forward(self,x):\n","    b,c,h,w = x.shape\n","    assert h == self.img_size and w == self.img_size\n","    x = self.proj(x).flatten(2).transpose(1,2) # b, ph*pw, c\n","    if self.norm is not None:\n","      x = self.norm(x)\n","    return x"]},{"cell_type":"code","source":["class PatchMergnig(nn.Module):\n","  def __init__(self, input_resolution, dim, norm_layer = nn.LayerNorm):\n","    super().__init__()\n","    self.input_resolution = input_resolution\n","    self.dim = dim\n","    self.reduction = nn.Linear(4*dim, 2*dim, bias = False)\n","    self.norm = norm_layer(4 * dim)\n","\n","  def forward(self, x):\n","    h, w = self.input_resolution\n","    b,l,c = x.shape\n","    assert l == h*w\n","    assert h % 2 == 0 and w % 2 == 0\n","\n","    x = x.view(b,h,w,c)\n","    x0 = x[:,0::2,0::2,:]# b h/2 w/2 c\n","    x1 = x[:,1::2,0::2,:]\n","    x2 = x[:,0::2,1::2,:]\n","    x3 = x[:,1::2,1::2,:]\n","    x = torch.cat([x0,x1,x2,x3], -1)# b h/2 w/2 c*4\n","    x = x.view(b,-1,4*c)\n","    x = self.norm(x)\n","    x = self.reduction(x)\n","\n","    return x"],"metadata":{"id":"G5eWS-6pKd65","executionInfo":{"status":"ok","timestamp":1645438057376,"user_tz":-540,"elapsed":239,"user":{"displayName":"유호준","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09086645003143885896"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["def window_partition(x, window_size):\n","  b,h,w,c = x.shape\n","  x = einops.rearrange(x,'b (h ph) (w pw) c -> (b h w) ph pw c',ph = window_size, pw = window_size)\n","  return x\n","def window_reverse(windows, window_size, h,w):\n","  x = einops.rearrange(windows, '(b h w) ph pw c -> b (h ph) (w pw) c', h = h//window_size, w = w//window_size, ph = window_size, pw= window_size)\n","  return x\n","\n","class SwinTransformerBlock(nn.Module):\n","  def __init__(self, dim, input_resolution, num_heads, window_size = 7, shift_size = 0, mlp_ratio=4.0,\n","              qkv_bias = True, qk_scale = None, drop = 0., attn_drop = 0., drop_path= 0., act_layer = nn.GELU, norm_layer=nn.LayerNorm):\n","    super().__init__()\n","    self.dim = dim\n","    self.input_resolution = input_resolution\n","    self.num_heads = num_heads\n","    self.window_size = window_size\n","    self.shift_size = shift_size\n","    self.mlp_ratio = mlp_ratio\n","    if min(self.input_resolution) <= self.window_size:\n","      self.shift_size = 0\n","      self.window_size = min(self.input_resolution)\n","    assert 0 <= self.shift_size < self.window_size\n","\n","    self.norm1 = norm_layer(dim)\n","    self.attn = WindowAttention(dim, window_size = self.window_size, num_heads = num_heads,\n","                                qkv_bias = qkv_bias, qk_scale = qk_scale, attn_drop = attn_drop, proj_drop = drop)\n","    self.drop_path  = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n","    self.norm2 = norm_layer(dim)\n","    mlp_hidden_dim = int(dim * mlp_ratio)\n","    self.mlp = Mlp(in_features = dim, hidden_features = mlp_hidden_dim, act_layer = act_layer, drop = drop)\n","\n","    if self.shift_size>0:\n","      #SW-MSA를 위한 마스크 계산\n","      H,W = self.input_resolution\n","      img_mask = torch.zeros((1,H,W,1))\n","      h_slices = (slice(0,-self.window_size),slice(-self.window_size,-self.shift_size),slice(-self.shift_size,None))\n","      w_slices = (slice(0,-self.window_size),slice(-self.window_size,-self.shift_size),slice(-self.shift_size,None))\n","      cnt = 0\n","                                                                                            \n","      for h in h_slices:\n","        for w in w_slices:\n","          img_mask[:,h,w,:] = cnt\n","          cnt+=1\n","      \n","      mask_windows = window_partition(img_mask,self.window_size)\n","      mask_windows = mask_windows.view(-1, self.window_size*self.window_size)\n","      attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n","      attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask ==0, float(0.0))\n","    else:\n","      attn_mask = None\n","    self.register_buffer('attn_mask',attn_mask)\n","  def forward(self,x):\n","    h,w = self.input_resolution\n","    b,l,c = x.shape # b = batch, l = seq_len, c = channel\n","    assert l == h*w\n","\n","    shortcut = x\n","    x = self.norm1(x)\n","    x = x.view(b,h,w,c)\n","\n","    if self.shift_size > 0:\n","      shifted_x = torch.roll(x, shifts=(-self.shift_size,-self.shift_size), dims = (1,2))\n","    else:\n","      shifted_x = x\n","\n","    #partition windows\n","    x_windows = window_partition(shifted_x, self.window_size) # N*W*B, window_size, window_size, c\n","    x_windows = x_windows.view(-1,self.window_size**2, c) #N*W*B, window_size * window_size, c\n","\n","    #W-MSA/SW-MSA\n","    attn_windows = self.attn(x_windows, mask = self.attn_mask)\n","\n","    #merge windows\n","    attn_windows = attn_windows.view(-1, self.window_size,self.window_size, c)\n","    shifted_x = window_reverse(attn_windows,self.window_size,h,w)\n","\n","    if self.shift_size>0:\n","      x = torch.roll(shifted_x, shifts = (self.shift_size, self.shift_size),dims = (1,2))\n","    else:\n","      x = shifted_x\n","    x = x.view(b,l,c)\n","\n","    x = shortcut + self.drop_path(x)\n","    x = x + self.drop_path(self.mlp(self.norm2(x)))\n","\n","    return x"],"metadata":{"id":"E33bRXNh8Sir","executionInfo":{"status":"ok","timestamp":1645446153883,"user_tz":-540,"elapsed":436,"user":{"displayName":"유호준","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09086645003143885896"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["class BasicLayer(nn.Module):\n","  def __init__(self, dim, input_resolution, depth, num_heads, window_size, mlp_ratio = 4.0, \n","               qkv_bias = True, qk_scale = True, drop = 0., attn_drop=0., drop_path = 0., \n","               norm_layer = nn.LayerNorm, downsample= None, use_checkpoint=False):\n","    super().__init__()\n","    self.dim = dim\n","    self.input_resolution = input_resolution\n","    self.depth = depth\n","    self.use_checkpoint = use_checkpoint\n","\n","    self.blocks = nn.ModuleList([\n","      SwinTransformerBlock(dim=dim, input_resolution = input_resolution, num_heads= num_heads, window_size=window_size, shift_size = 0 if (i % 2 ==0) else window_size // 2, mlp_ratio = mlp_ratio, qkv_bias=qkv_bias,\n","                      qk_scale=qk_scale, drop=drop, attn_drop = attn_drop, drop_path = drop_path[i] if isinstance(drop_path, list) else drop_path, norm_layer = norm_layer) for i in range(depth)\n","    ])\n","\n","    if downsample is not None:\n","      self.downsample = downsample(input_resolution, dim=dim, norm_layer = norm_layer)\n","    else: \n","      self.downsample = None\n","  def forward(self, x):\n","    for block in self.blocks:\n","      if self.use_checkpoint:\n","        x = checkpoint.checkpoint(block,x)\n","      else:\n","        x = block(x)\n","    \n","    if self.downsample is not None:\n","      x = self.downsample(x)\n","    return x"],"metadata":{"id":"p4SzLoqM7fr8","executionInfo":{"status":"ok","timestamp":1645437550054,"user_tz":-540,"elapsed":8,"user":{"displayName":"유호준","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09086645003143885896"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["class SwinTransformer(nn.Module):\n","  def __init__(self, img_size = 224, patch_size = 4, in_channels = 3, num_classes = 1000, embed_dim = 96, depths = [2,2,6,2], num_heads = [3,6,12,24],\n","               window_size = 7, mlp_ratio = 4.0, qkv_bias = True, qk_scale = None, drop_rate = 0, attn_drop_rate = 0., drop_path_rate = 0.1, norm_layer = nn.LayerNorm,\n","               ape = False, patch_norm = True, use_checkpoint = False, **kwargs):\n","    super().__init__()\n","    self.num_classes = num_classes\n","    self.num_layers = len(depths)\n","    self.embed_dim = embed_dim\n","    self.ape = ape\n","    self.patch_norm = patch_norm\n","    self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n","    self.mlp_ratio = mlp_ratio\n","\n","    self.patch_embed = PatchEmbed(img_size = img_size, patch_size = patch_size, in_channels=in_channels, embed_dim = embed_dim, norm_layer = norm_layer if self.patch_norm else None)\n","    num_patches = self.patch_embed.num_patches\n","    patch_resolution = self.patch_embed.patch_resolution\n","    self.patch_resolution = patch_resolution\n","\n","    #absolute position embedding\n","    if self.ape:\n","      self.absolute_pos_embed = nn.Parameter(torch.zeors(1,num_patches,embed_dim))\n","      trunc_normal_(self.absolute_pos_embed, std=.02)\n","\n","    self.pos_drop = nn.Dropout(drop_rate)\n","    #stochatic depth\n","    #depth를를 무작위로 드랍\n","    dpr = [x.item() for x in torch.linspace(0,drop_path_rate, sum(depths))]\n","\n","    self.layers = nn.ModuleList()\n","    for i_layer in range(self.num_layers):\n","      layer = BasicLayer(dim = int(embed_dim * 2 ** i_layer), input_resolution = (patch_resolution[0] // (2**i_layer), patch_resolution[1] // (2**i_layer)), depth = depths[i_layer],\n","                         num_heads = num_heads[i_layer], window_size = window_size, mlp_ratio = self.mlp_ratio, qkv_bias = qkv_bias, qk_scale = qk_scale, drop = drop_rate, attn_drop=attn_drop_rate,\n","                         drop_path = dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])], norm_layer = norm_layer, downsample = PatchMergnig if (i_layer < self.num_layers -1) else None, use_checkpoint = use_checkpoint)\n","      self.layers.append(layer)\n","    \n","    self.norm = norm_layer(self.num_features)\n","    self.avgpool = nn.AdaptiveAvgPool1d(1)\n","    self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n","    self.apply(self._init_weights)\n","  def _init_weights(self,m):\n","    if isinstance(m, nn.Linear):\n","      trunc_normal_(m.weight,std=0.02)\n","      if isinstance(m,nn.Linear) and m.bias is not None:\n","        nn.init.constant_(m.bias,0)\n","    elif isinstance(m,nn.LayerNorm):\n","      nn.init.constant_(m.bias,0)\n","      nn.init.constant_(m.weight,1.0)\n","  def forward(self,x):\n","    x = self.patch_embed(x)\n","    if self.ape:\n","      x = x + self.absolute_pos_embed\n","    x = self.pos_drop(x)\n","\n","    for layer in self.layers:\n","      x = layer(x)\n","\n","    x = self.norm(x)\n","    x = self.avgpool(x.transpose(1,2))\n","    x = torch.flatten(x,1)\n","    \n","    x = self.head(x)\n","    return x"],"metadata":{"id":"w3mTh6dNSvzv","executionInfo":{"status":"ok","timestamp":1645437550054,"user_tz":-540,"elapsed":5,"user":{"displayName":"유호준","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09086645003143885896"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["model = SwinTransformer().cuda()"],"metadata":{"id":"SxRbOFue6ogk","executionInfo":{"status":"ok","timestamp":1645438059879,"user_tz":-540,"elapsed":698,"user":{"displayName":"유호준","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09086645003143885896"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["summary(model,(3,224,224))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_GXaZv-0d84G","executionInfo":{"status":"ok","timestamp":1645438060135,"user_tz":-540,"elapsed":258,"user":{"displayName":"유호준","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09086645003143885896"}},"outputId":"25e457f6-dbb2-4cfd-cd3a-23cd4bbac138"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 64, 3, 49, 49]) torch.Size([1, 64, 1, 49, 49])\n","torch.Size([2, 16, 6, 49, 49]) torch.Size([1, 16, 1, 49, 49])\n","torch.Size([2, 4, 12, 49, 49]) torch.Size([1, 4, 1, 49, 49])\n","torch.Size([2, 4, 12, 49, 49]) torch.Size([1, 4, 1, 49, 49])\n","torch.Size([2, 4, 12, 49, 49]) torch.Size([1, 4, 1, 49, 49])\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 96, 56, 56]           4,704\n","         LayerNorm-2             [-1, 3136, 96]             192\n","        PatchEmbed-3             [-1, 3136, 96]               0\n","           Dropout-4             [-1, 3136, 96]               0\n","         LayerNorm-5             [-1, 3136, 96]             192\n","            Linear-6              [-1, 49, 288]          27,936\n","           Softmax-7            [-1, 3, 49, 49]               0\n","           Dropout-8            [-1, 3, 49, 49]               0\n","            Linear-9               [-1, 49, 96]           9,312\n","          Dropout-10               [-1, 49, 96]               0\n","  WindowAttention-11               [-1, 49, 96]               0\n","         Identity-12             [-1, 3136, 96]               0\n","        LayerNorm-13             [-1, 3136, 96]             192\n","           Linear-14            [-1, 3136, 384]          37,248\n","             GELU-15            [-1, 3136, 384]               0\n","          Dropout-16            [-1, 3136, 384]               0\n","           Linear-17             [-1, 3136, 96]          36,960\n","          Dropout-18             [-1, 3136, 96]               0\n","              Mlp-19             [-1, 3136, 96]               0\n","         Identity-20             [-1, 3136, 96]               0\n","SwinTransformerBlock-21             [-1, 3136, 96]               0\n","        LayerNorm-22             [-1, 3136, 96]             192\n","           Linear-23              [-1, 49, 288]          27,936\n","          Softmax-24            [-1, 3, 49, 49]               0\n","          Dropout-25            [-1, 3, 49, 49]               0\n","           Linear-26               [-1, 49, 96]           9,312\n","          Dropout-27               [-1, 49, 96]               0\n","  WindowAttention-28               [-1, 49, 96]               0\n","         DropPath-29             [-1, 3136, 96]               0\n","        LayerNorm-30             [-1, 3136, 96]             192\n","           Linear-31            [-1, 3136, 384]          37,248\n","             GELU-32            [-1, 3136, 384]               0\n","          Dropout-33            [-1, 3136, 384]               0\n","           Linear-34             [-1, 3136, 96]          36,960\n","          Dropout-35             [-1, 3136, 96]               0\n","              Mlp-36             [-1, 3136, 96]               0\n","         DropPath-37             [-1, 3136, 96]               0\n","SwinTransformerBlock-38             [-1, 3136, 96]               0\n","        LayerNorm-39             [-1, 784, 384]             768\n","           Linear-40             [-1, 784, 192]          73,728\n","     PatchMergnig-41             [-1, 784, 192]               0\n","       BasicLayer-42             [-1, 784, 192]               0\n","        LayerNorm-43             [-1, 784, 192]             384\n","           Linear-44              [-1, 49, 576]         111,168\n","          Softmax-45            [-1, 6, 49, 49]               0\n","          Dropout-46            [-1, 6, 49, 49]               0\n","           Linear-47              [-1, 49, 192]          37,056\n","          Dropout-48              [-1, 49, 192]               0\n","  WindowAttention-49              [-1, 49, 192]               0\n","         DropPath-50             [-1, 784, 192]               0\n","        LayerNorm-51             [-1, 784, 192]             384\n","           Linear-52             [-1, 784, 768]         148,224\n","             GELU-53             [-1, 784, 768]               0\n","          Dropout-54             [-1, 784, 768]               0\n","           Linear-55             [-1, 784, 192]         147,648\n","          Dropout-56             [-1, 784, 192]               0\n","              Mlp-57             [-1, 784, 192]               0\n","         DropPath-58             [-1, 784, 192]               0\n","SwinTransformerBlock-59             [-1, 784, 192]               0\n","        LayerNorm-60             [-1, 784, 192]             384\n","           Linear-61              [-1, 49, 576]         111,168\n","          Softmax-62            [-1, 6, 49, 49]               0\n","          Dropout-63            [-1, 6, 49, 49]               0\n","           Linear-64              [-1, 49, 192]          37,056\n","          Dropout-65              [-1, 49, 192]               0\n","  WindowAttention-66              [-1, 49, 192]               0\n","         DropPath-67             [-1, 784, 192]               0\n","        LayerNorm-68             [-1, 784, 192]             384\n","           Linear-69             [-1, 784, 768]         148,224\n","             GELU-70             [-1, 784, 768]               0\n","          Dropout-71             [-1, 784, 768]               0\n","           Linear-72             [-1, 784, 192]         147,648\n","          Dropout-73             [-1, 784, 192]               0\n","              Mlp-74             [-1, 784, 192]               0\n","         DropPath-75             [-1, 784, 192]               0\n","SwinTransformerBlock-76             [-1, 784, 192]               0\n","        LayerNorm-77             [-1, 196, 768]           1,536\n","           Linear-78             [-1, 196, 384]         294,912\n","     PatchMergnig-79             [-1, 196, 384]               0\n","       BasicLayer-80             [-1, 196, 384]               0\n","        LayerNorm-81             [-1, 196, 384]             768\n","           Linear-82             [-1, 49, 1152]         443,520\n","          Softmax-83           [-1, 12, 49, 49]               0\n","          Dropout-84           [-1, 12, 49, 49]               0\n","           Linear-85              [-1, 49, 384]         147,840\n","          Dropout-86              [-1, 49, 384]               0\n","  WindowAttention-87              [-1, 49, 384]               0\n","         DropPath-88             [-1, 196, 384]               0\n","        LayerNorm-89             [-1, 196, 384]             768\n","           Linear-90            [-1, 196, 1536]         591,360\n","             GELU-91            [-1, 196, 1536]               0\n","          Dropout-92            [-1, 196, 1536]               0\n","           Linear-93             [-1, 196, 384]         590,208\n","          Dropout-94             [-1, 196, 384]               0\n","              Mlp-95             [-1, 196, 384]               0\n","         DropPath-96             [-1, 196, 384]               0\n","SwinTransformerBlock-97             [-1, 196, 384]               0\n","        LayerNorm-98             [-1, 196, 384]             768\n","           Linear-99             [-1, 49, 1152]         443,520\n","         Softmax-100           [-1, 12, 49, 49]               0\n","         Dropout-101           [-1, 12, 49, 49]               0\n","          Linear-102              [-1, 49, 384]         147,840\n","         Dropout-103              [-1, 49, 384]               0\n"," WindowAttention-104              [-1, 49, 384]               0\n","        DropPath-105             [-1, 196, 384]               0\n","       LayerNorm-106             [-1, 196, 384]             768\n","          Linear-107            [-1, 196, 1536]         591,360\n","            GELU-108            [-1, 196, 1536]               0\n","         Dropout-109            [-1, 196, 1536]               0\n","          Linear-110             [-1, 196, 384]         590,208\n","         Dropout-111             [-1, 196, 384]               0\n","             Mlp-112             [-1, 196, 384]               0\n","        DropPath-113             [-1, 196, 384]               0\n","SwinTransformerBlock-114             [-1, 196, 384]               0\n","       LayerNorm-115             [-1, 196, 384]             768\n","          Linear-116             [-1, 49, 1152]         443,520\n","         Softmax-117           [-1, 12, 49, 49]               0\n","         Dropout-118           [-1, 12, 49, 49]               0\n","          Linear-119              [-1, 49, 384]         147,840\n","         Dropout-120              [-1, 49, 384]               0\n"," WindowAttention-121              [-1, 49, 384]               0\n","        DropPath-122             [-1, 196, 384]               0\n","       LayerNorm-123             [-1, 196, 384]             768\n","          Linear-124            [-1, 196, 1536]         591,360\n","            GELU-125            [-1, 196, 1536]               0\n","         Dropout-126            [-1, 196, 1536]               0\n","          Linear-127             [-1, 196, 384]         590,208\n","         Dropout-128             [-1, 196, 384]               0\n","             Mlp-129             [-1, 196, 384]               0\n","        DropPath-130             [-1, 196, 384]               0\n","SwinTransformerBlock-131             [-1, 196, 384]               0\n","       LayerNorm-132             [-1, 196, 384]             768\n","          Linear-133             [-1, 49, 1152]         443,520\n","         Softmax-134           [-1, 12, 49, 49]               0\n","         Dropout-135           [-1, 12, 49, 49]               0\n","          Linear-136              [-1, 49, 384]         147,840\n","         Dropout-137              [-1, 49, 384]               0\n"," WindowAttention-138              [-1, 49, 384]               0\n","        DropPath-139             [-1, 196, 384]               0\n","       LayerNorm-140             [-1, 196, 384]             768\n","          Linear-141            [-1, 196, 1536]         591,360\n","            GELU-142            [-1, 196, 1536]               0\n","         Dropout-143            [-1, 196, 1536]               0\n","          Linear-144             [-1, 196, 384]         590,208\n","         Dropout-145             [-1, 196, 384]               0\n","             Mlp-146             [-1, 196, 384]               0\n","        DropPath-147             [-1, 196, 384]               0\n","SwinTransformerBlock-148             [-1, 196, 384]               0\n","       LayerNorm-149             [-1, 196, 384]             768\n","          Linear-150             [-1, 49, 1152]         443,520\n","         Softmax-151           [-1, 12, 49, 49]               0\n","         Dropout-152           [-1, 12, 49, 49]               0\n","          Linear-153              [-1, 49, 384]         147,840\n","         Dropout-154              [-1, 49, 384]               0\n"," WindowAttention-155              [-1, 49, 384]               0\n","        DropPath-156             [-1, 196, 384]               0\n","       LayerNorm-157             [-1, 196, 384]             768\n","          Linear-158            [-1, 196, 1536]         591,360\n","            GELU-159            [-1, 196, 1536]               0\n","         Dropout-160            [-1, 196, 1536]               0\n","          Linear-161             [-1, 196, 384]         590,208\n","         Dropout-162             [-1, 196, 384]               0\n","             Mlp-163             [-1, 196, 384]               0\n","        DropPath-164             [-1, 196, 384]               0\n","SwinTransformerBlock-165             [-1, 196, 384]               0\n","       LayerNorm-166             [-1, 196, 384]             768\n","          Linear-167             [-1, 49, 1152]         443,520\n","         Softmax-168           [-1, 12, 49, 49]               0\n","         Dropout-169           [-1, 12, 49, 49]               0\n","          Linear-170              [-1, 49, 384]         147,840\n","         Dropout-171              [-1, 49, 384]               0\n"," WindowAttention-172              [-1, 49, 384]               0\n","        DropPath-173             [-1, 196, 384]               0\n","       LayerNorm-174             [-1, 196, 384]             768\n","          Linear-175            [-1, 196, 1536]         591,360\n","            GELU-176            [-1, 196, 1536]               0\n","         Dropout-177            [-1, 196, 1536]               0\n","          Linear-178             [-1, 196, 384]         590,208\n","         Dropout-179             [-1, 196, 384]               0\n","             Mlp-180             [-1, 196, 384]               0\n","        DropPath-181             [-1, 196, 384]               0\n","SwinTransformerBlock-182             [-1, 196, 384]               0\n","       LayerNorm-183             [-1, 49, 1536]           3,072\n","          Linear-184              [-1, 49, 768]       1,179,648\n","    PatchMergnig-185              [-1, 49, 768]               0\n","      BasicLayer-186              [-1, 49, 768]               0\n","       LayerNorm-187              [-1, 49, 768]           1,536\n","          Linear-188             [-1, 49, 2304]       1,771,776\n","         Softmax-189           [-1, 24, 49, 49]               0\n","         Dropout-190           [-1, 24, 49, 49]               0\n","          Linear-191              [-1, 49, 768]         590,592\n","         Dropout-192              [-1, 49, 768]               0\n"," WindowAttention-193              [-1, 49, 768]               0\n","        DropPath-194              [-1, 49, 768]               0\n","       LayerNorm-195              [-1, 49, 768]           1,536\n","          Linear-196             [-1, 49, 3072]       2,362,368\n","            GELU-197             [-1, 49, 3072]               0\n","         Dropout-198             [-1, 49, 3072]               0\n","          Linear-199              [-1, 49, 768]       2,360,064\n","         Dropout-200              [-1, 49, 768]               0\n","             Mlp-201              [-1, 49, 768]               0\n","        DropPath-202              [-1, 49, 768]               0\n","SwinTransformerBlock-203              [-1, 49, 768]               0\n","       LayerNorm-204              [-1, 49, 768]           1,536\n","          Linear-205             [-1, 49, 2304]       1,771,776\n","         Softmax-206           [-1, 24, 49, 49]               0\n","         Dropout-207           [-1, 24, 49, 49]               0\n","          Linear-208              [-1, 49, 768]         590,592\n","         Dropout-209              [-1, 49, 768]               0\n"," WindowAttention-210              [-1, 49, 768]               0\n","        DropPath-211              [-1, 49, 768]               0\n","       LayerNorm-212              [-1, 49, 768]           1,536\n","          Linear-213             [-1, 49, 3072]       2,362,368\n","            GELU-214             [-1, 49, 3072]               0\n","         Dropout-215             [-1, 49, 3072]               0\n","          Linear-216              [-1, 49, 768]       2,360,064\n","         Dropout-217              [-1, 49, 768]               0\n","             Mlp-218              [-1, 49, 768]               0\n","        DropPath-219              [-1, 49, 768]               0\n","SwinTransformerBlock-220              [-1, 49, 768]               0\n","      BasicLayer-221              [-1, 49, 768]               0\n","       LayerNorm-222              [-1, 49, 768]           1,536\n","AdaptiveAvgPool1d-223               [-1, 768, 1]               0\n","          Linear-224                 [-1, 1000]         769,000\n","================================================================\n","Total params: 28,265,032\n","Trainable params: 28,265,032\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.57\n","Forward/backward pass size (MB): 252.99\n","Params size (MB): 107.82\n","Estimated Total Size (MB): 361.39\n","----------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["relative_position_bias_table = nn.Parameter(\n","            torch.zeros((2 * 3 - 1) * (2 * 3 - 1), 1))  # 2*Wh-1 * 2*Ww-1, nH\n","\n","    # get pair-wise relative position index for each token inside the window\n","coords_h = torch.arange(3)\n","coords_w = torch.arange(3)\n","coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n","coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n","relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n","\n","print('############## coords_flatten ##############')\n","print(einops.repeat(coords_flatten.unsqueeze(2),'h l ()-> h l n', n=9))\n","print(einops.repeat(coords_flatten.unsqueeze(1),'h () l -> h n l', n=9))\n","print('############## coords_flatten ##############')\n","print('############## coords_flatten -  coords_flatten ##############')\n","relative_coords = einops.repeat(coords_flatten.unsqueeze(2),'h l ()-> h l n', n=9) - einops.repeat(coords_flatten.unsqueeze(1),'h () l -> h n l', n=9)\n","print(relative_coords)\n","print('############## coords_flatten -  coords_flatten ##############')\n","relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n","relative_coords[:, :, 0] += 3 - 1  # shift to start from 0\n","print('############## relative_coords x ##############')\n","print(relative_coords[:, :, 0])\n","print('############## relative_coords x ##############')\n","relative_coords[:, :, 1] += 3 - 1\n","print('############## relative_coords y ##############')\n","print(relative_coords[:, :, 1])\n","print('############## relative_coords y ##############')\n","relative_coords[:, :, 0] *= 2 * 3 - 1\n","\n","relative_position_index = relative_coords.sum(-1)\n","\n","print('############## relative_coords y ##############')\n","print(relative_position_index)\n","print('############## relative_coords y ##############')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0TUCwjudxw96","executionInfo":{"status":"ok","timestamp":1645440774327,"user_tz":-540,"elapsed":368,"user":{"displayName":"유호준","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09086645003143885896"}},"outputId":"17801632-281d-4db4-e77a-fcd4e37d2f93"},"execution_count":61,"outputs":[{"output_type":"stream","name":"stdout","text":["############## coords_flatten ##############\n","tensor([[[0, 0, 0, 0, 0, 0, 0, 0, 0],\n","         [0, 0, 0, 0, 0, 0, 0, 0, 0],\n","         [0, 0, 0, 0, 0, 0, 0, 0, 0],\n","         [1, 1, 1, 1, 1, 1, 1, 1, 1],\n","         [1, 1, 1, 1, 1, 1, 1, 1, 1],\n","         [1, 1, 1, 1, 1, 1, 1, 1, 1],\n","         [2, 2, 2, 2, 2, 2, 2, 2, 2],\n","         [2, 2, 2, 2, 2, 2, 2, 2, 2],\n","         [2, 2, 2, 2, 2, 2, 2, 2, 2]],\n","\n","        [[0, 0, 0, 0, 0, 0, 0, 0, 0],\n","         [1, 1, 1, 1, 1, 1, 1, 1, 1],\n","         [2, 2, 2, 2, 2, 2, 2, 2, 2],\n","         [0, 0, 0, 0, 0, 0, 0, 0, 0],\n","         [1, 1, 1, 1, 1, 1, 1, 1, 1],\n","         [2, 2, 2, 2, 2, 2, 2, 2, 2],\n","         [0, 0, 0, 0, 0, 0, 0, 0, 0],\n","         [1, 1, 1, 1, 1, 1, 1, 1, 1],\n","         [2, 2, 2, 2, 2, 2, 2, 2, 2]]])\n","tensor([[[0, 0, 0, 1, 1, 1, 2, 2, 2],\n","         [0, 0, 0, 1, 1, 1, 2, 2, 2],\n","         [0, 0, 0, 1, 1, 1, 2, 2, 2],\n","         [0, 0, 0, 1, 1, 1, 2, 2, 2],\n","         [0, 0, 0, 1, 1, 1, 2, 2, 2],\n","         [0, 0, 0, 1, 1, 1, 2, 2, 2],\n","         [0, 0, 0, 1, 1, 1, 2, 2, 2],\n","         [0, 0, 0, 1, 1, 1, 2, 2, 2],\n","         [0, 0, 0, 1, 1, 1, 2, 2, 2]],\n","\n","        [[0, 1, 2, 0, 1, 2, 0, 1, 2],\n","         [0, 1, 2, 0, 1, 2, 0, 1, 2],\n","         [0, 1, 2, 0, 1, 2, 0, 1, 2],\n","         [0, 1, 2, 0, 1, 2, 0, 1, 2],\n","         [0, 1, 2, 0, 1, 2, 0, 1, 2],\n","         [0, 1, 2, 0, 1, 2, 0, 1, 2],\n","         [0, 1, 2, 0, 1, 2, 0, 1, 2],\n","         [0, 1, 2, 0, 1, 2, 0, 1, 2],\n","         [0, 1, 2, 0, 1, 2, 0, 1, 2]]])\n","############## coords_flatten ##############\n","############## coords_flatten -  coords_flatten ##############\n","tensor([[[ 0,  0,  0, -1, -1, -1, -2, -2, -2],\n","         [ 0,  0,  0, -1, -1, -1, -2, -2, -2],\n","         [ 0,  0,  0, -1, -1, -1, -2, -2, -2],\n","         [ 1,  1,  1,  0,  0,  0, -1, -1, -1],\n","         [ 1,  1,  1,  0,  0,  0, -1, -1, -1],\n","         [ 1,  1,  1,  0,  0,  0, -1, -1, -1],\n","         [ 2,  2,  2,  1,  1,  1,  0,  0,  0],\n","         [ 2,  2,  2,  1,  1,  1,  0,  0,  0],\n","         [ 2,  2,  2,  1,  1,  1,  0,  0,  0]],\n","\n","        [[ 0, -1, -2,  0, -1, -2,  0, -1, -2],\n","         [ 1,  0, -1,  1,  0, -1,  1,  0, -1],\n","         [ 2,  1,  0,  2,  1,  0,  2,  1,  0],\n","         [ 0, -1, -2,  0, -1, -2,  0, -1, -2],\n","         [ 1,  0, -1,  1,  0, -1,  1,  0, -1],\n","         [ 2,  1,  0,  2,  1,  0,  2,  1,  0],\n","         [ 0, -1, -2,  0, -1, -2,  0, -1, -2],\n","         [ 1,  0, -1,  1,  0, -1,  1,  0, -1],\n","         [ 2,  1,  0,  2,  1,  0,  2,  1,  0]]])\n","############## coords_flatten -  coords_flatten ##############\n","############## relative_coords x ##############\n","tensor([[2, 2, 2, 1, 1, 1, 0, 0, 0],\n","        [2, 2, 2, 1, 1, 1, 0, 0, 0],\n","        [2, 2, 2, 1, 1, 1, 0, 0, 0],\n","        [3, 3, 3, 2, 2, 2, 1, 1, 1],\n","        [3, 3, 3, 2, 2, 2, 1, 1, 1],\n","        [3, 3, 3, 2, 2, 2, 1, 1, 1],\n","        [4, 4, 4, 3, 3, 3, 2, 2, 2],\n","        [4, 4, 4, 3, 3, 3, 2, 2, 2],\n","        [4, 4, 4, 3, 3, 3, 2, 2, 2]])\n","############## relative_coords x ##############\n","############## relative_coords y ##############\n","tensor([[2, 1, 0, 2, 1, 0, 2, 1, 0],\n","        [3, 2, 1, 3, 2, 1, 3, 2, 1],\n","        [4, 3, 2, 4, 3, 2, 4, 3, 2],\n","        [2, 1, 0, 2, 1, 0, 2, 1, 0],\n","        [3, 2, 1, 3, 2, 1, 3, 2, 1],\n","        [4, 3, 2, 4, 3, 2, 4, 3, 2],\n","        [2, 1, 0, 2, 1, 0, 2, 1, 0],\n","        [3, 2, 1, 3, 2, 1, 3, 2, 1],\n","        [4, 3, 2, 4, 3, 2, 4, 3, 2]])\n","############## relative_coords y ##############\n","############## relative_coords y ##############\n","tensor([[12, 11, 10,  7,  6,  5,  2,  1,  0],\n","        [13, 12, 11,  8,  7,  6,  3,  2,  1],\n","        [14, 13, 12,  9,  8,  7,  4,  3,  2],\n","        [17, 16, 15, 12, 11, 10,  7,  6,  5],\n","        [18, 17, 16, 13, 12, 11,  8,  7,  6],\n","        [19, 18, 17, 14, 13, 12,  9,  8,  7],\n","        [22, 21, 20, 17, 16, 15, 12, 11, 10],\n","        [23, 22, 21, 18, 17, 16, 13, 12, 11],\n","        [24, 23, 22, 19, 18, 17, 14, 13, 12]])\n","############## relative_coords y ##############\n"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","img_mask = torch.zeros((1,8,8,1))\n","h_slices = (slice(0,-4),slice(-4,-2),slice(-2,None))\n","w_slices = (slice(0,-4),slice(-4,-2),slice(-2,None))\n","cnt = 0\n","                                                                                            \n","for h in h_slices:\n","  for w in w_slices:\n","    img_mask[:,h,w,:] = cnt\n","    cnt+=1\n","\n","print(img_mask.squeeze(0).squeeze(-1))      \n","mask_windows = window_partition(img_mask,4)#패치 단위로 나눔\n","mask_windows = mask_windows.view(-1, 4*4)\n","#둘이 빼서 0이라면? 원래 같은 부분 -> attention 계산\n","#둘이 빼서 1이라면? 다른 부분 -> attention 계산 안하기 위해 마스킹 -> -100\n","attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n","\n","\n","attn_mask = attn_mask.masked_fill(attn_mask != 0, float(255.0)).masked_fill(attn_mask ==0, float(0.0))\n","\n","plt.figure(1, figsize=(20,10))\n","plt.subplot(1,4,1)\n","plt.imshow(attn_mask[0], cmap='gray')\n","plt.subplot(1,4,2)\n","plt.imshow(attn_mask[1], cmap='gray')\n","plt.subplot(1,4,3)\n","plt.imshow(attn_mask[2], cmap='gray')\n","plt.subplot(1,4,4)\n","plt.imshow(attn_mask[3], cmap='gray')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":444},"id":"jQoFogsW_9dM","executionInfo":{"status":"ok","timestamp":1645447135560,"user_tz":-540,"elapsed":727,"user":{"displayName":"유호준","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09086645003143885896"}},"outputId":"951eaeb2-50c1-41d7-85e1-9af3894a0184"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0., 0., 0., 0., 1., 1., 2., 2.],\n","        [0., 0., 0., 0., 1., 1., 2., 2.],\n","        [0., 0., 0., 0., 1., 1., 2., 2.],\n","        [0., 0., 0., 0., 1., 1., 2., 2.],\n","        [3., 3., 3., 3., 4., 4., 5., 5.],\n","        [3., 3., 3., 3., 4., 4., 5., 5.],\n","        [6., 6., 6., 6., 7., 7., 8., 8.],\n","        [6., 6., 6., 6., 7., 7., 8., 8.]])\n"]},{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7f42cd38b690>"]},"metadata":{},"execution_count":41},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAABH4AAAESCAYAAACLuxKfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAXzklEQVR4nO3db4zteV0f8Pene6W4aAXKFXV36W4MoSHEFu6NQW2sEWpWJK4P+gAiBirJPmkrGhoCktT0UZto/JPUaDaAl7RkfYBYCYmWLTUhTZA4d/m3sCgUKey6uJeQqtEHsPHTB3Pudu7snTvnzjnn9/vt97xeyeTOOXNmPt/vzJn3/vY9v3NOdXcAAAAAGM/fm3sBAAAAAOyG4gcAAABgUIofAAAAgEEpfgAAAAAGpfgBAAAAGJTiBwAAAGBQ56YcVlVeOx7G8NXuPj/3Is5qriy6cOHCHGOTJJcvX55l7lx7nmu/iT1P6fLly7KIRZsz95mOLDobx0WwXTfKouqe7vfcAQ4M43J3X5x7EWc1VxZNmbfHVdUsc+fa81z7Tex5SlUli1i0OXOf6ciis3FcBNt1oyzyUC8AAACAQSl+AAAAAAa1UfFTVXdX1Z9U1eer6q3bWhTAzZBFwFLII2AJZBFw1JmLn6q6JcmvJ/nRJC9O8tqqevG2FgawDlkELIU8ApZAFgHHbXLGz/cm+Xx3f6G7v57kt5Pcs51lAaxNFgFLIY+AJZBFwDU2KX5uS/LlI5cfWV0HMCVZBCyFPAKWQBYB1zi36wFVdW+Se3c9B+BGZBGwBLIIWAJZBPtlk+Ln0SR3HLl8++q6a3T3fUnuS5Kq6g3mAVyPLAKW4tQ8kkXABGQRcI1NHur1x0leWFV3VdUzkrwmyfu3syyAtckiYCnkEbAEsgi4xpnP+OnuJ6rq3yT570luSfKu7v701lYGsAZZBCyFPAKWQBYBx1X3dGf2OY0QhnG5uy/OvYizmiuLpszb46pqlrlz7Xmu/Sb2PKWqkkUs2py5z3Rk0dk4LoLtulEWbfJQLwAAAAAWTPEDAAAAMCjFDwAAAMCgFD8AAAAAg1L8AAAAAAxK8QMAAAAwKMUPAAAAwKAUPwAAAACDUvwAAAAADErxAwAAADAoxQ8AAADAoBQ/AAAAAINS/AAAAAAMSvEDAAAAMCjFDwAAAMCgFD8AAAAAg1L8AAAAAAzq3NwLAJjahQsXcnBwMPncqpp85lXdPcvcufY8134Tewbg6cVx0XT28RiBZXDGDwAAAMCgFD8AAAAAg1L8AAAAAAzqzMVPVd1RVX9YVZ+pqk9X1Zu2uTCAdcgiYCnkEbAEsgg4bpMnd34iyZu7+8Gq+tYkl6vqge7+zJbWBrAOWQQshTwClkAWAdc48xk/3f1Ydz+4ev+vkzyc5LZtLQxgHbIIWAp5BCyBLAKO28pz/FTVnUlemuSj2/h6AGchi4ClkEfAEsgiINlC8VNV35Lkd5L8bHf/1XU+fm9VHVTVwaazAE5yM1l05cqV6RcI7I0b5ZHjImAq62aR4yIY30bFT1V9Uw7D5D3d/b7r3aa77+vui919cZNZACe52Sw6f/78tAsE9sZpeeS4CJjCzWSR4yIY3yav6lVJ3pnk4e7+5e0tCWB9sghYCnkELIEsAo7b5IyfH0jyU0l+uKo+vnp71ZbWBbAuWQQshTwClkAWAdc488u5d/f/SlJbXAvATZNFwFLII2AJZBFw3FZe1QsAAACA5VH8AAAAAAxK8QMAAAAwKMUPAAAAwKAUPwAAAACDUvwAAAAADErxAwAAADAoxQ8AAADAoBQ/AAAAAINS/AAAAAAMSvEDAAAAMCjFDwAAAMCgFD8AAAAAg1L8AAAAAAxK8QMAAAAwKMUPAAAAwKAUPwAAAACDUvwAAAAADOrc3AsAmNrly5dTVZPP7e7JZ141x36T+fY8134Te57SnHsGGIXjounMuWf2mzN+AAAAAAal+AEAAAAY1MbFT1XdUlUfq6oPbGNBAGchi4AlkEXAUsgj4KptnPHzpiQPb+HrAGxCFgFLIIuApZBHQJINi5+quj3JjyV5x3aWA3DzZBGwBLIIWAp5BBy16Rk/v5rkLUn+bgtrATgrWQQsgSwClkIeAU86c/FTVa9O8nh3Xz7ldvdW1UFVHZx1FsBJZBGwBLIIWIp18kgWwX6p7j7bJ1b9xyQ/leSJJM9M8g+SvK+7X3eDzznbMGBpLnf3xbkXkTy9suisebsNVTXL3Ln2PNd+E3ueUlXJIhZtztxnOkvKouTm88hx0XRkArt0oyw6c/FzbMAPJfl33f3qU27nng5jWNQBzlVLzyIHONPZ0xJklrmJ4ue4pWcR0/E/efthqVmUrJdHjoumIxPYpRtl0TZe1QsAAACABdrKGT9rD/OXLRjFYv+ytQ5/2ZqOs1+ms6d7lkUsmr/u7wdZdDaOi2C7nPEDAAAAsIcUPwAAAACDUvwAAAAADErxAwAAADAoxQ8AAADAoBQ/AAAAAINS/AAAAAAMSvEDAAAAMCjFDwAAAMCgFD8AAAAAg1L8AAAAAAxK8QMAAAAwKMUPAAAAwKAUPwAAAACDUvwAAAAADErxAwAAADAoxQ8AAADAoBQ/AAAAAIM6N/cCAKZ24cKFHBwcTD63qiafeVV3zzJ3rj3Ptd/EngF4enFcNJ19PEZgGZzxAwAAADAoxQ8AAADAoBQ/AAAAAIPaqPipqmdX1Xur6rNV9XBVfd+2FgawLlkELIU8ApZAFgFHbfrkzr+W5A+6+19W1TOS3LqFNQHcLFkELIU8ApZAFgFPOnPxU1XfluQHk7whSbr760m+vp1lAaxHFgFLIY+AJZBFwHGbPNTrriRXkvxWVX2sqt5RVc/a0roA1iWLgKWQR8ASyCLgGpsUP+eSvCzJb3T3S5P8TZK3Hr9RVd1bVQdVdbDBLICT3HQWXblyZeo1Avvh1DxyXARM4KayyHERjG+T4ueRJI9090dXl9+bw4C5Rnff190Xu/viBrMATnLTWXT+/PlJFwjsjVPzyHERMIGbyiLHRTC+Mxc/3f2VJF+uqhetrnpFks9sZVUAa5JFwFLII2AJZBFw3Kav6vVvk7xn9UzxX0jyrzZfEsBNk0XAUsgjYAlkEfCkjYqf7v54EqcqA7OSRcBSyCNgCWQRcNQmz/EDAAAAwIIpfgAAAAAGpfgBAAAAGJTiBwAAAGBQih8AAACAQSl+AAAAAAal+AEAAAAYlOIHAAAAYFCKHwAAAIBBKX4AAAAABqX4AQAAABiU4gcAAABgUIofAAAAgEEpfgAAAAAGpfgBAAAAGJTiBwAAAGBQih8AAACAQZ2bewEAU7t8+XKqavK53T35zKvm2G8y357n2m9iz1Oac8/bcOHChRwcHMy9DHbo6X4fhV2a87hoLvv430t7XgZn/AAAAAAMSvEDAAAAMCjFDwAAAMCgNip+qurnqurTVfVQVd1fVc/c1sIA1iWLgKWQR8ASyCLgqDMXP1V1W5KfSXKxu1+S5JYkr9nWwgDWIYuApZBHwBLIIuC4TR/qdS7JN1fVuSS3JvnzzZcEcNNkEbAU8ghYAlkEPOnMxU93P5rkl5J8KcljSf6yuz+4rYUBrEMWAUshj4AlkEXAcZs81Os5Se5JcleS70ryrKp63XVud29VHVTVwdmXCXB9sghYinXy6GgWXblyZY5lAoOTRcBxmzzU65VJ/qy7r3T3N5K8L8n3H79Rd9/X3Re7++IGswBOIouApTg1j45m0fnz52dZJDA8WQRcY5Pi50tJXl5Vt1ZVJXlFkoe3syyAtckiYCnkEbAEsgi4xibP8fPRJO9N8mCST62+1n1bWhfAWmQRsBTyCFgCWQQcd26TT+7uX0jyC1taC8CZyCJgKeQRsASyCDhq05dzBwAAAGChFD8AAAAAg1L8AAAAAAxK8QMAAAAwKMUPAAAAwKAUPwAAAACDUvwAAAAADErxAwAAADAoxQ8AAADAoBQ/AAAAAINS/AAAAAAMSvEDAAAAMCjFDwAAAMCgFD8AAAAAg1L8AAAAAAxK8QMAAAAwKMUPAAAAwKAUPwAAAACDOjf3AgCmduHChRwcHEw+t6omn3lVd88yd649z7XfxJ4BgGXZx2OEfdzzjTjjBwAAAGBQih8AAACAQZ1a/FTVu6rq8ap66Mh1z62qB6rqc6t/n7PbZQLII2AZZBGwBLIIWNc6Z/xcSnL3sevemuRD3f3CJB9aXQbYtUuRR8D8LkUWAfO7FFkErOHU4qe7P5zka8euvifJu1fvvzvJT2x5XQBPIY+AJZBFwBLIImBdZ32On+d392Or97+S5PlbWg/AzZJHwBLIImAJZBHwFBs/uXMfvk7aia+VVlX3VtVBVU3/2snAXrlRHh3NoitXrky8MmCfyCJgCWQRcNVZi5+/qKrvTJLVv4+fdMPuvq+7L3b3xTPOAriRtfLoaBadP39+0gUCe0EWAUsgi4CnOGvx8/4kr1+9//okv7ed5QDcNHkELIEsApZAFgFPsc7Lud+f5CNJXlRVj1TVG5P8pyT/oqo+l+SVq8sAOyWPgCWQRcASyCJgXedOu0F3v/aED71iy2sBuCF5BCyBLAKWQBYB69r4yZ0BAAAAWCbFDwAAAMCgFD8AAAAAg1L8AAAAAAxK8QMAAAAwKMUPAAAAwKAUPwAAAACDUvwAAAAADErxAwAAADAoxQ8AAADAoBQ/AAAAAINS/AAAAAAMSvEDAAAAMCjFDwAAAMCgFD8AAAAAg1L8AAAAAAxK8QMAAAAwKMUPAAAAwKDOzb0AgKldvnw5VTX53O6efOZVc+w3mW/Pc+03secpzblnAHi6cIwwrSXu2Rk/AAAAAINS/AAAAAAMSvEDAAAAMKhTi5+qeldVPV5VDx257her6rNV9cmq+t2qevZulwnsO1kELIU8ApZAFgHrWueMn0tJ7j523QNJXtLd35PkT5O8bcvrAjjuUmQRsAyXIo+A+V2KLALWcGrx090fTvK1Y9d9sLufWF38oyS372BtAE+SRcBSyCNgCWQRsK5tPMfPTyf5/S18HYBNyCJgKeQRsASyCEiyYfFTVW9P8kSS99zgNvdW1UFVHWwyC+AksghYitPy6GgWXblyZdrFAXtDFgFHnbn4qao3JHl1kp/s7j7pdt19X3df7O6LZ50FcBJZBCzFOnl0NIvOnz8/6fqA/SCLgOPOneWTquruJG9J8s+7+2+3uySA9cgiYCnkEbAEsgi4nnVezv3+JB9J8qKqeqSq3pjkPyf51iQPVNXHq+o3d7xOYM/JImAp5BGwBLIIWNepZ/x092uvc/U7d7AWgBPJImAp5BGwBLIIWNc2XtULAAAAgAVS/AAAAAAMSvEDAAAAMCjFDwAAAMCgFD8AAAAAg1L8AAAAAAxK8QMAAAAwKMUPAAAAwKAUPwAAAACDUvwAAAAADErxAwAAADAoxQ8AAADAoBQ/AAAAAINS/AAAAAAMSvEDAAAAMCjFDwAAAMCgFD8AAAAAg1L8AAAAAAzq3NwLAJjahQsXcnBwMPncqpp85lXdPcvcufY8134TewYAlmUfjxH2cc834owfAAAAgEEpfgAAAAAGdWrxU1XvqqrHq+qh63zszVXVVfW83SwP4P+TR8ASyCJgCWQRsK51zvi5lOTu41dW1R1JfiTJl7a8JoCTXIo8AuZ3KbIImN+lyCJgDacWP9394SRfu86HfiXJW5LM96xJwF6RR8ASyCJgCWQRsK4zPcdPVd2T5NHu/sSW1wNwU+QRsASyCFgCWQRcz02/nHtV3Zrk53N4+uA6t783yb03OwfgNDeTR0ez6AUveMGOVwbsE1kELIEsAk5yljN+vjvJXUk+UVVfTHJ7kger6juud+Puvq+7L3b3xbMvE+C61s6jo1l0/vz5iZcJDE4WAUsgi4Druukzfrr7U0m+/erlVahc7O6vbnFdAKeSR8ASyCJgCWQRcJJ1Xs79/iQfSfKiqnqkqt64+2UBPJU8ApZAFgFLIIuAdZ16xk93v/aUj9+5tdUA3IA8ApZAFgFLIIuAdZ3pVb0AAAAAWD7FDwAAAMCgFD8AAAAAg1L8AAAAAAxK8QMAAAAwKMUPAAAAwKAUPwAAAACDUvwAAAAADErxAwAAADAoxQ8AAADAoBQ/AAAAAINS/AAAAAAMSvEDAAAAMCjFDwAAAMCgFD8AAAAAg1L8AAAAAAxK8QMAAAAwKMUPAAAAwKCqu6cbVnUlyf+5wU2el+SrEy1nn+fOOdvcMWb/o+4+v6OvvXOyaDFz55y9b3PnnC2LTiCLFjN3ztn7NnfO2bLoBLJoUbP3be6cs0ece2IWTVr8nKaqDrr7ornjzjZ3P2Y/3e3b/WUf76f7NnfO2bLo7PbtZ+b3Y/y5c86WRWe3jz+zfduz7/X4cz3UCwAAAGBQih8AAACAQS2t+LnP3OFnm7sfs5/u9u3+so/3032bO+dsWXR2+/Yz8/sx/tw5Z8uis9vHn9m+7dn3evC5i3qOHwAAAAC2Z2ln/AAAAACwJYsofqrq7qr6k6r6fFW9daKZd1TVH1bVZ6rq01X1pinmHpl/S1V9rKo+MPHcZ1fVe6vqs1X1cFV930Rzf271fX6oqu6vqmfucNa7qurxqnroyHXPraoHqupzq3+fM9HcX1x9rz9ZVb9bVc/e9tyTZh/52JurqqvqebuYPZI5smg1d+/ySBbJIk4mi8bPotXsSfJIFj3lY7JoTbJIFm15zixZdIPZO8+jJWXR7MVPVd2S5NeT/GiSFyd5bVW9eILRTyR5c3e/OMnLk/zrieZe9aYkD08476pfS/IH3f2Pk/yTKdZQVbcl+ZkkF7v7JUluSfKaHY68lOTuY9e9NcmHuvuFST60ujzF3AeSvKS7vyfJnyZ52w7mnjQ7VXVHkh9J8qUdzR3GjFmU7GceySJZxHXIovGzKJk8jy5FFiWRRTdDFsmiHbiUebLopNlT5NH15s6SRbMXP0m+N8nnu/sL3f31JL+d5J5dD+3ux7r7wdX7f53DX67bdj03Sarq9iQ/luQdU8w7MvfbkvxgkncmSXd/vbv/70TjzyX55qo6l+TWJH++q0Hd/eEkXzt29T1J3r16/91JfmKKud39we5+YnXxj5Lcvu25J81e+ZUkb0niybxON0sWJfuXR7JIFnFDsmgiM2dRMlEeyaJryKL1yaKJyKLdZtFJs6fIoyVl0RKKn9uSfPnI5Ucy0S/2VVV1Z5KXJvnoRCN/NYc/6L+baN5VdyW5kuS3VqcvvqOqnrXrod39aJJfymGj+ViSv+zuD+567jHP7+7HVu9/JcnzJ56fJD+d5PenGlZV9yR5tLs/MdXMp7nZsyjZmzySRYdkEdcji6YzSxYli8gjWcRpZNF0ZNGhubIomTCP5sqiJRQ/s6qqb0nyO0l+trv/aoJ5r07yeHdf3vWs6ziX5GVJfqO7X5rkb7K70+metHqs5j05DLXvSvKsqnrdrueepA9fym7Sv/RU1dtzeNrqeyaad2uSn0/y76eYx3bsUR7JosgilksW7d6S8kgWsVSyaPf2PYuSafNozixaQvHzaJI7jly+fXXdzlXVN+UwTN7T3e+bYmaSH0jy41X1xRyeMvnDVfVfJ5r9SJJHuvtqY/7eHIbMrr0yyZ9195Xu/kaS9yX5/gnmHvUXVfWdSbL69/GpBlfVG5K8OslPrgJtCt+dwwD/xOq+dnuSB6vqOyaa/3Q0WxYle5dHsiiyaKL5T0eyaPwsSubPI1kki04ji2TRFGbLotXMN2TaPJoti5ZQ/PxxkhdW1V1V9YwcPpnU+3c9tKoqh4+jfLi7f3nX867q7rd19+3dfWcO9/o/u3uSVrW7v5Lky1X1otVVr0jymQlGfynJy6vq1tX3/RWZ/gnT3p/k9av3X5/k96YYWlV35/B00R/v7r+dYmaSdPenuvvbu/vO1X3tkSQvW90HuL5ZsijZvzySRbJIFt2QLBo/i5L580gWyaLTyCJZNIVZsiiZJ49mzaLunv0tyaty+Eza/zvJ2yea+c9yeCrZJ5N8fPX2qon3/UNJPjDxzH+a5GC17/+W5DkTzf0PST6b5KEk/yXJ39/hrPtz+BjVb+Twl+mNSf5hDp8p/nNJ/keS50409/M5fHz01fvYb06152Mf/2KS5015X3s6vs2RRau5e5dHskgWebvh91EWTTdvlixazZ4kj2SRLNrg+yiLppsni3aURTeYvfM8WlIW1WogAAAAAINZwkO9AAAAANgBxQ8AAADAoBQ/AAAAAINS/AAAAAAMSvEDAAAAMCjFDwAAAMCgFD8AAAAAg1L8AAAAAAzq/wHGg3noG/Ex/wAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 1440x720 with 4 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["depths = [2,2,6,2]\n","\n","print(torch.linspace(0,0.1, sum([2,2,6,2])))\n","dpr = [x.item() for x in torch.linspace(0,0.1, sum([2,2,6,2]))]\n","print(dpr)\n","\n","print(len(dpr))\n","print(dpr[sum(depths[:1]):sum(depths[:1 + 1])])\n","dpr[sum(depths[:0]):sum(depths[:0 + 1])]\n","\n","print(depths[:3])\n","\n","print(depths[:4 + 1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qTGgx-QXHHv6","executionInfo":{"status":"ok","timestamp":1645444739407,"user_tz":-540,"elapsed":244,"user":{"displayName":"유호준","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09086645003143885896"}},"outputId":"2229322c-8717-40f2-bfe2-9645e3bc69fd"},"execution_count":82,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0.0000, 0.0091, 0.0182, 0.0273, 0.0364, 0.0455, 0.0545, 0.0636, 0.0727,\n","        0.0818, 0.0909, 0.1000])\n","[0.0, 0.00909090880304575, 0.0181818176060915, 0.027272727340459824, 0.036363635212183, 0.045454543083906174, 0.054545458406209946, 0.06363636255264282, 0.0727272778749466, 0.08181818574666977, 0.09090909361839294, 0.10000000149011612]\n","12\n","[0.0181818176060915, 0.027272727340459824]\n","[2, 2, 6]\n","[2, 2, 6, 2]\n"]}]},{"cell_type":"code","source":["x = torch.zeros(8,8)\n","window_size = 4\n","cnt = 0\n","for h in range(2):\n","  for w in range(2):\n","    x[h*window_size:(h+1)*window_size,w*window_size:(w+1)*window_size] = cnt\n","    cnt+=1\n","\n","print(x)\n","print(torch.roll(x, shifts = (-2, -2),dims = (0,1)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vFXDOMoRJ0Su","executionInfo":{"status":"ok","timestamp":1645445714243,"user_tz":-540,"elapsed":404,"user":{"displayName":"유호준","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09086645003143885896"}},"outputId":"1b71f902-523c-4d4b-b212-e56fbc9c8d20"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0., 0., 0., 0., 1., 1., 1., 1.],\n","        [0., 0., 0., 0., 1., 1., 1., 1.],\n","        [0., 0., 0., 0., 1., 1., 1., 1.],\n","        [0., 0., 0., 0., 1., 1., 1., 1.],\n","        [2., 2., 2., 2., 3., 3., 3., 3.],\n","        [2., 2., 2., 2., 3., 3., 3., 3.],\n","        [2., 2., 2., 2., 3., 3., 3., 3.],\n","        [2., 2., 2., 2., 3., 3., 3., 3.]])\n","tensor([[0., 0., 1., 1., 1., 1., 0., 0.],\n","        [0., 0., 1., 1., 1., 1., 0., 0.],\n","        [2., 2., 3., 3., 3., 3., 2., 2.],\n","        [2., 2., 3., 3., 3., 3., 2., 2.],\n","        [2., 2., 3., 3., 3., 3., 2., 2.],\n","        [2., 2., 3., 3., 3., 3., 2., 2.],\n","        [0., 0., 1., 1., 1., 1., 0., 0.],\n","        [0., 0., 1., 1., 1., 1., 0., 0.]])\n"]}]}]}